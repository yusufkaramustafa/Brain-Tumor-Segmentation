{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2542390,"sourceType":"datasetVersion","datasetId":1541666}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nibabel as nib\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom skimage.transform import rotate\nfrom skimage.util import montage\nimport os\nimport numpy as np\nimport tarfile\nimport pandas as pd\nfrom skimage.transform import resize\nfrom scipy.ndimage import gaussian_filter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:15:58.286575Z","iopub.execute_input":"2024-12-08T09:15:58.286968Z","iopub.status.idle":"2024-12-08T09:15:59.527791Z","shell.execute_reply.started":"2024-12-08T09:15:58.286934Z","shell.execute_reply":"2024-12-08T09:15:59.526568Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"path_to_tar_file = \"/kaggle/input/brats-2021-task1/BraTS2021_Training_Data.tar\"\noutput_directory = \"brats21-dataset-training-validation\"\n\n# Open the tar file in read mode\nwith tarfile.open(path_to_tar_file, \"r\") as tar_ref:\n    # Extract all contents to the specified directory\n    tar_ref.extractall(output_directory)\n\nprint(f\"Extraction completed. Files are saved to {output_directory}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:16:00.702675Z","iopub.execute_input":"2024-12-08T09:16:00.703265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport nibabel as nib\n\n\nclass Brats2021LSTMDataset(Dataset):\n    def __init__(self, data_dir=None, slice_window=3, train=True, transform=None):\n        \"\"\"\n        Dataset for BraTS2021 with LSTM-CNN using lazy loading.\n        Args:\n            data_dir (str): Path to dataset directory.\n            slice_window (int): Number of consecutive slices for LSTM input.\n            train (bool): Whether in training mode.\n            transform (callable): Transformations (e.g., normalization, augmentation).\n        \"\"\"\n        self.data_dir = data_dir\n        self.slice_window = slice_window\n        self.train = train\n        self.transform = transform  # Augmentation pipeline\n        self.modalities = ['flair', 't1', 't1ce', 't2']\n\n        # Collect paths for all subject directories\n        if data_dir:\n            self.subject_paths = [os.path.join(data_dir, subject) for subject in os.listdir(data_dir)\n                                  if os.path.isdir(os.path.join(data_dir, subject))]\n        else:\n            self.subject_paths = []\n\n    def __len__(self):\n        return len(self.subject_paths)\n\n    def __getitem__(self, index):\n        # Load the subject data lazily\n        subject_path = self.subject_paths[index]\n        volume, label = self._load_subject(subject_path)\n    \n        # Normalize\n        volume = self._normalize_volume(volume)\n    \n        if self.train:\n            # Randomly sample slices for training\n            depth = volume.shape[3]\n            for _ in range(10):\n                i = np.random.randint(depth - self.slice_window + 1)\n                slice_vol = volume[:, :, :, i:i + self.slice_window]\n                slice_label = label[:, :, i:i + self.slice_window]\n                if not np.all(slice_vol == 0):\n                    break\n            else:\n                raise ValueError(f\"All slices are zero for subject {subject_path}\")\n    \n            # Apply augmentation pipeline\n            if self.transform:\n                slice_vol, slice_label = self.apply_transforms(slice_vol, slice_label)\n    \n            slice_vol = torch.tensor(slice_vol, dtype=torch.float32).permute(3,0,1,2)\n            slice_label = torch.tensor(slice_label, dtype=torch.long).permute(2,0,1)\n            \n            return slice_vol, slice_label\n    \n        else:\n            # Return full volume for evaluation\n            volume = torch.tensor(volume, dtype=torch.float32).permute(3,0,1,2)  # Shape: (depth, modalities, height, width,)\n            label = torch.tensor(label, dtype=torch.long).permute(2,0,1)    # Shape: (depth, height, width)\n    \n            return volume, label \n\n    def _load_subject(self, subject_path):\n        \"\"\"\n        Load modalities and segmentation labels for a given subject.\n        Args:\n            subject_path (str): Path to the subject's directory.\n        Returns:\n            volume (np.array): Array of shape (modalities, height, width, depth).\n            label (np.array): Array of shape (height,width, depth).\n        \"\"\"\n        volume = []\n        label = None\n        subject_name = os.path.basename(subject_path)\n\n        for modal in self.modalities:\n            modal_path = os.path.join(subject_path, f\"{subject_name}_{modal}.nii.gz\")\n            if os.path.exists(modal_path):\n                volume.append(nib.load(modal_path, mmap=True).get_fdata())\n            else:\n                raise FileNotFoundError(f\"Missing modality: {modal_path}\")\n\n        label_path = os.path.join(subject_path, f\"{subject_name}_seg.nii.gz\")\n        if os.path.exists(label_path):\n            label = nib.load(label_path, mmap=True).get_fdata()\n        else:\n            raise FileNotFoundError(f\"Missing segmentation: {label_path}\")\n\n        \n        return np.asarray(volume), label\n\n    @staticmethod\n    def _normalize_volume(volume):\n        \"\"\"\n        Normalize the volume to [0, 1].\n        \"\"\"\n        return (volume - np.min(volume)) / (np.max(volume) - np.min(volume) + 1e-8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:15:52.551636Z","iopub.status.idle":"2024-12-08T09:15:52.552205Z","shell.execute_reply.started":"2024-12-08T09:15:52.551915Z","shell.execute_reply":"2024-12-08T09:15:52.551944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport os\n\n# Directory containing all data\ndata_dir = '/kaggle/working/brats21-dataset-training-validation'\n\n# List all subject directories\nall_subjects = [os.path.join(data_dir, subject) for subject in os.listdir(data_dir)\n                if os.path.isdir(os.path.join(data_dir, subject))]\n\n# Split into train, validation, and test sets\ntrain_subjects, temp_subjects = train_test_split(all_subjects, test_size=0.3, random_state=42)\nval_subjects, test_subjects = train_test_split(temp_subjects, test_size=0.3, random_state=42)\n\n# Print sizes of each split\nprint(f\"Total Subjects: {len(all_subjects)}\")\nprint(f\"Training: {len(train_subjects)}, Validation: {len(val_subjects)}, Testing: {len(test_subjects)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:15:52.556129Z","iopub.status.idle":"2024-12-08T09:15:52.556636Z","shell.execute_reply.started":"2024-12-08T09:15:52.556386Z","shell.execute_reply":"2024-12-08T09:15:52.556418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = '/kaggle/working/brats21-dataset-training-validation'\n\n# Training dataset with augmentation\ntrain_dataset = Brats2021LSTMDataset(\n    data_dir= data_dir,\n    slice_window=3,\n    train=True,\n)\ntrain_dataset.subject_paths = train_subjects\n\n# Validation dataset without augmentation\nval_dataset = Brats2021LSTMDataset(\n    data_dir= data_dir,\n    slice_window=3,\n    train=False,\n)\nval_dataset.subject_paths = val_subjects\n\n# Initialize testing dataset\ntest_dataset = Brats2021LSTMDataset(\n    data_dir=None,\n    slice_window=3,\n    train=False,\n)\ntest_dataset.subject_paths = test_subjects  \n\nfrom torch.utils.data import DataLoader\n\n# Training DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\n\n# Validation DataLoader\nval_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2)\n\n# Testing DataLoader\ntest_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2)\n\n# Example training loop\nfor batch_idx, (images, labels) in enumerate(train_loader):\n    print(f\"Batch {batch_idx}:\")\n    print(f\"Images shape: {images.shape}\") \n    print(f\"Labels shape: {labels.shape}\")\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T09:15:52.558794Z","iopub.status.idle":"2024-12-08T09:15:52.559264Z","shell.execute_reply.started":"2024-12-08T09:15:52.559066Z","shell.execute_reply":"2024-12-08T09:15:52.559087Z"}},"outputs":[],"execution_count":null}]}