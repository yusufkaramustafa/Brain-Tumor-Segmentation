{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10171132,"sourceType":"datasetVersion","datasetId":6281714},{"sourceId":201090,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":171558,"modelId":193875}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-17T13:45:56.933124Z","iopub.execute_input":"2024-12-17T13:45:56.933790Z","iopub.status.idle":"2024-12-17T13:45:57.694529Z","shell.execute_reply.started":"2024-12-17T13:45:56.933742Z","shell.execute_reply":"2024-12-17T13:45:57.693866Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class Brats2021LSTMDatasetLazy(Dataset):\n    def __init__(self, subject_files, temporal=4, train=True):\n        \"\"\"\n        subject_files: list of paths to preprocessed .npz files,\n                       each containing 'volume' and 'label' arrays.\n        temporal: number of slices per temporal window (for training)\n        train: whether this is a training set or not\n        \"\"\"\n        self.subject_files = subject_files\n        self.temporal = temporal\n        self.is_train = train\n\n        self.freq = np.zeros(5)\n        self.data = []\n\n        print('~' * 50)\n        print('******** Loading data********')\n        \n        count = 0\n        # First pass: compute frequency and determine slices/entries\n        for file_path in self.subject_files:\n            count += 1\n            if count % 10 == 0:\n                print('Processing subject %d/%d' % (count, len(self.subject_files)))\n            \n            volume, label = self._load_subject(file_path)\n            self.freq += self._get_freq(label) # Store the label of each subject to get the distributon and then calculate weights\n            \n            if self.is_train:\n                # Training: sample temporal windows\n                img_starts = self._compute_training_slices(volume)\n                for start in img_starts:\n                    self.data.append((file_path, start))\n            else:\n                # Validation/test: store entire subject\n                self.data.append((file_path, None))\n        \n        self.freq = self.freq / np.sum(self.freq)\n        self.weight = np.median(self.freq) / (self.freq + 1e-8) # Median weight used, same as the one proposed in the original paper\n        print('******** Finished setup ********')\n        print('Number of entries in this split:', len(self.data))\n        print('********  Temporal length is %d ********' % self.temporal)\n        print('********  Weight for all classes  ********')\n        print(self.weight)\n        print('~' * 50)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        \"\"\"\n        For training:\n            We have (file_path, start_slice) and we load just that temporal window.\n        For testing:\n            We have (file_path, None) and we load the full subject.\n        \"\"\"\n        file_path, start = self.data[index]\n        volume, label = self._load_subject(file_path)\n\n        if self.is_train:\n            # Extract the temporal window\n            images_vol, labels_vol, names = self._extract_window(volume, label, file_path, start)\n        else:\n            # Return full volume\n            # volume: (4,D,H,W), label: (D,H,W)\n            # For testing, we return (D,4,H,W)\n            volume = volume.transpose(1,0,2,3)  # (D,4,H,W)\n            images_vol = volume\n            labels_vol = label\n            names = [file_path]\n\n        return images_vol, labels_vol, names\n\n    def _load_subject(self, file_path):\n        \"\"\"\n        Loads a subject from a .npz file.\n        The .npz file must contain:\n          volume: (4,D,H,W) preprocessed volume\n          label:  (D,H,W) segmentation labels\n        \"\"\"\n        data = np.load(file_path)\n        volume = data['volume']  # (4,D,H,W)\n        label = data['label']    # (D,H,W)\n        return volume, label\n\n    def _compute_training_slices(self, volume):\n        \"\"\"\n        Determine where to sample temporal windows from.\n        Similar logic as the original paper:\n        Find non-zero start and end, then pick random windows.\n        \"\"\"\n        D = volume.shape[1]\n        H = volume.shape[2]\n        W = volume.shape[3]\n\n        zero_vol = np.zeros((4,H,W))\n        no_zero_start = 0\n        no_zero_end = D\n\n        # We keep checking until we find a non-zero start and end, by non-zero start we mean a slice that doesn't contain only the background\n        for i in range(D):\n            if not np.all(volume[:, i, :, :] == zero_vol):\n                no_zero_start = i\n                break\n\n        for i in range(no_zero_start, D):\n            if np.all(volume[:, i, :, :] == zero_vol):\n                no_zero_end = i\n                break\n\n        num_slices = no_zero_end - no_zero_start\n        times = int(num_slices / self.temporal)\n        # Random offset\n        rand_start = no_zero_start + np.random.randint(-self.temporal//2, self.temporal//2 + 1)\n\n        img_starts = []\n        for t in range(times):\n            start_idx = rand_start + t * self.temporal\n            # Adjust start_idx if it goes out of range\n            start_idx = max(start_idx, no_zero_start)\n            if start_idx + self.temporal <= no_zero_end:\n                img_starts.append(start_idx)\n\n        return img_starts\n\n    def _extract_window(self, volume, label, file_path, start):\n        \"\"\"\n        Extract a temporal window starting at 'start', start is a list containing start locations.\n        Returns (temporal,4,H,W) volume and (temporal,H,W) label.\n        \"\"\"\n        tmp_im = []\n        tmp_lbl = []\n        tmp_name = []\n        # extract slices from the depth starting at the determined spots in the last function\n        for i in range(start, start + self.temporal):\n            tmp_im.append(torch.from_numpy(volume[:, i, :, :]))   # (4,H,W)\n            tmp_lbl.append(torch.from_numpy(label[i, :, :]))       # (H,W)\n            tmp_name.append(file_path + '=slice' + str(i))\n\n        tmp_im = torch.stack(tmp_im)   # (temporal,4,H,W)\n        tmp_lbl = torch.stack(tmp_lbl) # (temporal,H,W)\n        return tmp_im, tmp_lbl, tmp_name\n\n    @staticmethod\n    def _get_freq(label):\n        \"\"\"\n        Calculate frequency of each class in the label.\n        label: (D,H,W)\n        \"\"\"\n        class_count = np.zeros((5))\n        for i in range(5):\n            class_count[i] = np.sum(label == i)\n        return class_count\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T13:45:57.696062Z","iopub.execute_input":"2024-12-17T13:45:57.696428Z","iopub.status.idle":"2024-12-17T13:46:00.814745Z","shell.execute_reply.started":"2024-12-17T13:45:57.696401Z","shell.execute_reply":"2024-12-17T13:46:00.813868Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Get all subject directories\ndata_dir = '/kaggle/input/brats2021-preprocessed/content/brats21-dataset-preprocessed'\nall_subjects = [os.path.join(data_dir, s) for s in os.listdir(data_dir) \n                if s.endswith('.npz')]\n\n\nnp.random.seed(42)  # for reproducibility\nnp.random.shuffle(all_subjects)\n\n# Split ratios\ntrain_ratio = 0.7\nval_ratio = 0.15\ntest_ratio = 0.15\n\ntotal = len(all_subjects)\ntrain_count = int(train_ratio * total)\nval_count = int(val_ratio * total)\n\ntrain_subjects = all_subjects[:train_count]\nval_subjects = all_subjects[train_count:train_count+val_count]\ntest_subjects = all_subjects[train_count+val_count:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T13:46:00.815951Z","iopub.execute_input":"2024-12-17T13:46:00.816462Z","iopub.status.idle":"2024-12-17T13:46:00.843303Z","shell.execute_reply.started":"2024-12-17T13:46:00.816423Z","shell.execute_reply":"2024-12-17T13:46:00.842465Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Create dataset objects for each split\ntrain_dataset = Brats2021LSTMDatasetLazy(train_subjects, temporal=4, train=True)\nval_dataset = Brats2021LSTMDatasetLazy(val_subjects, temporal=4, train=False)\ntest_dataset = Brats2021LSTMDatasetLazy(test_subjects, temporal=4, train=False)\n\n# Wrap them in DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, pin_memory = True, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=4, pin_memory = True, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=4, pin_memory = True, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T13:46:00.845444Z","iopub.execute_input":"2024-12-17T13:46:00.845783Z","iopub.status.idle":"2024-12-17T14:00:11.821547Z","shell.execute_reply.started":"2024-12-17T13:46:00.845746Z","shell.execute_reply":"2024-12-17T14:00:11.820586Z"}},"outputs":[{"name":"stdout","text":"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n******** Loading data in lazy mode (from NPZ) ********\nProcessing subject 10/875\nProcessing subject 20/875\nProcessing subject 30/875\nProcessing subject 40/875\nProcessing subject 50/875\nProcessing subject 60/875\nProcessing subject 70/875\nProcessing subject 80/875\nProcessing subject 90/875\nProcessing subject 100/875\nProcessing subject 110/875\nProcessing subject 120/875\nProcessing subject 130/875\nProcessing subject 140/875\nProcessing subject 150/875\nProcessing subject 160/875\nProcessing subject 170/875\nProcessing subject 180/875\nProcessing subject 190/875\nProcessing subject 200/875\nProcessing subject 210/875\nProcessing subject 220/875\nProcessing subject 230/875\nProcessing subject 240/875\nProcessing subject 250/875\nProcessing subject 260/875\nProcessing subject 270/875\nProcessing subject 280/875\nProcessing subject 290/875\nProcessing subject 300/875\nProcessing subject 310/875\nProcessing subject 320/875\nProcessing subject 330/875\nProcessing subject 340/875\nProcessing subject 350/875\nProcessing subject 360/875\nProcessing subject 370/875\nProcessing subject 380/875\nProcessing subject 390/875\nProcessing subject 400/875\nProcessing subject 410/875\nProcessing subject 420/875\nProcessing subject 430/875\nProcessing subject 440/875\nProcessing subject 450/875\nProcessing subject 460/875\nProcessing subject 470/875\nProcessing subject 480/875\nProcessing subject 490/875\nProcessing subject 500/875\nProcessing subject 510/875\nProcessing subject 520/875\nProcessing subject 530/875\nProcessing subject 540/875\nProcessing subject 550/875\nProcessing subject 560/875\nProcessing subject 570/875\nProcessing subject 580/875\nProcessing subject 590/875\nProcessing subject 600/875\nProcessing subject 610/875\nProcessing subject 620/875\nProcessing subject 630/875\nProcessing subject 640/875\nProcessing subject 650/875\nProcessing subject 660/875\nProcessing subject 670/875\nProcessing subject 680/875\nProcessing subject 690/875\nProcessing subject 700/875\nProcessing subject 710/875\nProcessing subject 720/875\nProcessing subject 730/875\nProcessing subject 740/875\nProcessing subject 750/875\nProcessing subject 760/875\nProcessing subject 770/875\nProcessing subject 780/875\nProcessing subject 790/875\nProcessing subject 800/875\nProcessing subject 810/875\nProcessing subject 820/875\nProcessing subject 830/875\nProcessing subject 840/875\nProcessing subject 850/875\nProcessing subject 860/875\nProcessing subject 870/875\n******** Finished setup ********\nNumber of entries in this split: 33250\n********  Temporal length is 4 ********\n********  Weight for all classes  ********\n[2.46444095e-03 1.50269941e+00 3.58035199e-01 2.43765681e+05\n 9.99995898e-01]\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n******** Loading data in lazy mode (from NPZ) ********\nProcessing subject 10/187\nProcessing subject 20/187\nProcessing subject 30/187\nProcessing subject 40/187\nProcessing subject 50/187\nProcessing subject 60/187\nProcessing subject 70/187\nProcessing subject 80/187\nProcessing subject 90/187\nProcessing subject 100/187\nProcessing subject 110/187\nProcessing subject 120/187\nProcessing subject 130/187\nProcessing subject 140/187\nProcessing subject 150/187\nProcessing subject 160/187\nProcessing subject 170/187\nProcessing subject 180/187\n******** Finished setup ********\nNumber of entries in this split: 187\n********  Temporal length is 4 ********\n********  Weight for all classes  ********\n[2.23186651e-03 1.34604724e+00 3.32258068e-01 2.20844115e+05\n 9.99995472e-01]\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n******** Loading data in lazy mode (from NPZ) ********\nProcessing subject 10/189\nProcessing subject 20/189\nProcessing subject 30/189\nProcessing subject 40/189\nProcessing subject 50/189\nProcessing subject 60/189\nProcessing subject 70/189\nProcessing subject 80/189\nProcessing subject 90/189\nProcessing subject 100/189\nProcessing subject 110/189\nProcessing subject 120/189\nProcessing subject 130/189\nProcessing subject 140/189\nProcessing subject 150/189\nProcessing subject 160/189\nProcessing subject 170/189\nProcessing subject 180/189\n******** Finished setup ********\nNumber of entries in this split: 189\n********  Temporal length is 4 ********\n********  Weight for all classes  ********\n[2.45576478e-03 1.65024324e+00 3.71285850e-01 2.43010753e+05\n 9.99995885e-01]\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Defining blocks\n#ConvBlock2d\nclass ConvBlock2d(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(ConvBlock2d, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\n# ConvTrans2d\nclass ConvTrans2d(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(ConvTrans2d, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.ConvTranspose2d(\n                in_ch, out_ch, kernel_size=3, stride=2, padding=1, output_padding=1\n            ),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv1(x)\n\n#UpBlock2d\nclass UpBlock2d(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(UpBlock2d, self).__init__()\n        self.up_conv = ConvTrans2d(in_ch, out_ch)\n        self.conv = ConvBlock2d(2 * out_ch, out_ch)\n\n    def forward(self, x, down_features):\n        x = self.up_conv(x)  # Upsample\n        x = torch.cat([x, down_features], dim=1)  # Concatenate with features from encoder\n        x = self.conv(x)  # Apply convolutions\n        return x\n\n#MaxPool2d\nclass MaxPool2d(nn.Module):\n    def __init__(self, kernel_size=2, stride=2, padding=0):\n        super(MaxPool2d, self).__init__()\n        self.pool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x):\n        return self.pool(x)\n\n#ConvBlock\nclass ConvBlock(nn.Module):\n    def __init__(self, in_dim, out_dim, act_fn, kernel_size=3, stride=1, padding=1):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_dim, out_dim, kernel_size=kernel_size, stride=stride, padding=padding),\n            nn.BatchNorm2d(out_dim),\n            act_fn,\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n        \ndef croppCenter(tensorToCrop,finalShape):\n    org_shape = tensorToCrop.shape\n\n    diff = np.zeros(2)\n    diff[0] = org_shape[2] - finalShape[2]\n    diff[1] = org_shape[3] - finalShape[3]\n\n    croppBorders = np.zeros(2,dtype=int)\n    croppBorders[0] = int(diff[0]/2)\n    croppBorders[1] = int(diff[1]/2)\n\n    return tensorToCrop[:, :,\n                        croppBorders[0]:croppBorders[0] + finalShape[2],\n                        croppBorders[1]:croppBorders[1] + finalShape[3]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T14:00:11.822624Z","iopub.execute_input":"2024-12-17T14:00:11.822914Z","iopub.status.idle":"2024-12-17T14:00:11.836283Z","shell.execute_reply.started":"2024-12-17T14:00:11.822887Z","shell.execute_reply":"2024-12-17T14:00:11.835326Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class MMUnet(nn.Module):\n    \"\"\"Multi-Modal-Unet\"\"\"\n    def __init__(self, input_nc, output_nc=5, ngf=32):\n        super(MMUnet, self).__init__()\n        print('~' * 50)\n        print(' ----- Creating MULTI_UNET  ...')\n        print('~' * 50)\n\n        self.in_dim = input_nc\n        self.out_dim = ngf\n        self.final_out_dim = output_nc\n\n        # Define the max pooling operation using the provided MaxPool2d class\n        self.maxpool = MaxPool2d(kernel_size=2, stride=2)\n\n        # ~~~ Encoding Paths ~~~~~~ #\n        # Encoder (Modality 1)\n        self.down_1_0 = ConvBlock2d(1, self.out_dim)\n        self.pool_1_0 = self.maxpool\n        self.down_2_0 = ConvBlock2d(self.out_dim * 4, self.out_dim * 2)\n        self.pool_2_0 = self.maxpool\n        self.down_3_0 = ConvBlock2d(self.out_dim * 12, self.out_dim * 4)\n        self.pool_3_0 = self.maxpool\n        self.down_4_0 = ConvBlock2d(self.out_dim * 28, self.out_dim * 8)\n        self.pool_4_0 = self.maxpool\n\n        # Encoder (Modality 2)\n        self.down_1_1 = ConvBlock2d(1, self.out_dim)\n        self.pool_1_1 = self.maxpool\n        self.down_2_1 = ConvBlock2d(self.out_dim * 4, self.out_dim * 2)\n        self.pool_2_1 = self.maxpool\n        self.down_3_1 = ConvBlock2d(self.out_dim * 12, self.out_dim * 4)\n        self.pool_3_1 = self.maxpool\n        self.down_4_1 = ConvBlock2d(self.out_dim * 28, self.out_dim * 8)\n        self.pool_4_1 = self.maxpool\n\n        # Encoder (Modality 3)\n        self.down_1_2 = ConvBlock2d(1, self.out_dim)\n        self.pool_1_2 = self.maxpool\n        self.down_2_2 = ConvBlock2d(self.out_dim * 4, self.out_dim * 2)\n        self.pool_2_2 = self.maxpool\n        self.down_3_2 = ConvBlock2d(self.out_dim * 12, self.out_dim * 4)\n        self.pool_3_2 = self.maxpool\n        self.down_4_2 = ConvBlock2d(self.out_dim * 28, self.out_dim * 8)\n        self.pool_4_2 = self.maxpool\n\n        # Encoder (Modality 4)\n        self.down_1_3 = ConvBlock2d(1, self.out_dim)\n        self.pool_1_3 = self.maxpool\n        self.down_2_3 = ConvBlock2d(self.out_dim * 4, self.out_dim * 2)\n        self.pool_2_3 = self.maxpool\n        self.down_3_3 = ConvBlock2d(self.out_dim * 12, self.out_dim * 4)\n        self.pool_3_3 = self.maxpool\n        self.down_4_3 = ConvBlock2d(self.out_dim * 28, self.out_dim * 8)\n        self.pool_4_3 = self.maxpool\n\n        # Bridge\n        self.bridge = ConvBlock2d(self.out_dim * 60, self.out_dim * 16)\n\n        # ~~~ Decoding Path ~~~~~~ #\n        self.upLayer1 = UpBlock2d(self.out_dim * 16, self.out_dim * 8)\n        self.upLayer2 = UpBlock2d(self.out_dim * 8, self.out_dim * 4)\n        self.upLayer3 = UpBlock2d(self.out_dim * 4, self.out_dim * 2)\n        self.upLayer4 = UpBlock2d(self.out_dim * 2, self.out_dim * 1)\n\n    def forward(self, input):\n        # Split input by modality\n        i0 = input[:, 0:1, :, :]\n        i1 = input[:, 1:2, :, :]\n        i2 = input[:, 2:3, :, :]\n        i3 = input[:, 3:4, :, :]\n\n        # First level\n        down_1_0 = self.down_1_0(i0)\n        down_1_1 = self.down_1_1(i1)\n        down_1_2 = self.down_1_2(i2)\n        down_1_3 = self.down_1_3(i3)\n\n        # Second level inputs\n        input_2nd_0 = torch.cat((self.pool_1_0(down_1_0),\n                                 self.pool_1_1(down_1_1),\n                                 self.pool_1_2(down_1_2),\n                                 self.pool_1_3(down_1_3)), dim=1)\n\n        input_2nd_1 = torch.cat((self.pool_1_1(down_1_1),\n                                 self.pool_1_2(down_1_2),\n                                 self.pool_1_3(down_1_3),\n                                 self.pool_1_0(down_1_0)), dim=1)\n\n        input_2nd_2 = torch.cat((self.pool_1_2(down_1_2),\n                                 self.pool_1_3(down_1_3),\n                                 self.pool_1_0(down_1_0),\n                                 self.pool_1_1(down_1_1)), dim=1)\n\n        input_2nd_3 = torch.cat((self.pool_1_3(down_1_3),\n                                 self.pool_1_0(down_1_0),\n                                 self.pool_1_1(down_1_1),\n                                 self.pool_1_2(down_1_2)), dim=1)\n\n        down_2_0 = self.down_2_0(input_2nd_0)\n        down_2_1 = self.down_2_1(input_2nd_1)\n        down_2_2 = self.down_2_2(input_2nd_2)\n        down_2_3 = self.down_2_3(input_2nd_3)\n\n        # Third level inputs\n        down_2_0m = self.pool_2_0(down_2_0)\n        down_2_1m = self.pool_2_1(down_2_1)\n        down_2_2m = self.pool_2_2(down_2_2)\n        down_2_3m = self.pool_2_3(down_2_3)\n\n        input_3rd_0 = torch.cat((down_2_0m, down_2_1m, down_2_2m, down_2_3m), dim=1)\n        input_3rd_0 = torch.cat((input_3rd_0, croppCenter(input_2nd_0, input_3rd_0.shape)), dim=1)\n\n        input_3rd_1 = torch.cat((down_2_1m, down_2_2m, down_2_3m, down_2_0m), dim=1)\n        input_3rd_1 = torch.cat((input_3rd_1, croppCenter(input_2nd_1, input_3rd_1.shape)), dim=1)\n\n        input_3rd_2 = torch.cat((down_2_2m, down_2_3m, down_2_0m, down_2_1m), dim=1)\n        input_3rd_2 = torch.cat((input_3rd_2, croppCenter(input_2nd_2, input_3rd_2.shape)), dim=1)\n\n        input_3rd_3 = torch.cat((down_2_3m, down_2_0m, down_2_1m, down_2_2m), dim=1)\n        input_3rd_3 = torch.cat((input_3rd_3, croppCenter(input_2nd_3, input_3rd_3.shape)), dim=1)\n\n        down_3_0 = self.down_3_0(input_3rd_0)\n        down_3_1 = self.down_3_1(input_3rd_1)\n        down_3_2 = self.down_3_2(input_3rd_2)\n        down_3_3 = self.down_3_3(input_3rd_3)\n\n        # Fourth level inputs\n        down_3_0m = self.pool_3_0(down_3_0)\n        down_3_1m = self.pool_3_1(down_3_1)\n        down_3_2m = self.pool_3_2(down_3_2)\n        down_3_3m = self.pool_3_3(down_3_3)\n\n        input_4th_0 = torch.cat((down_3_0m, down_3_1m, down_3_2m, down_3_3m), dim=1)\n        input_4th_0 = torch.cat((input_4th_0, croppCenter(input_3rd_0, input_4th_0.shape)), dim=1)\n\n        input_4th_1 = torch.cat((down_3_1m, down_3_2m, down_3_3m, down_3_0m), dim=1)\n        input_4th_1 = torch.cat((input_4th_1, croppCenter(input_3rd_1, input_4th_1.shape)), dim=1)\n\n        input_4th_2 = torch.cat((down_3_2m, down_3_3m, down_3_0m, down_3_1m), dim=1)\n        input_4th_2 = torch.cat((input_4th_2, croppCenter(input_3rd_2, input_4th_2.shape)), dim=1)\n\n        input_4th_3 = torch.cat((down_3_3m, down_3_0m, down_3_1m, down_3_2m), dim=1)\n        input_4th_3 = torch.cat((input_4th_3, croppCenter(input_3rd_3, input_4th_3.shape)), dim=1)\n\n        down_4_0 = self.down_4_0(input_4th_0)\n        down_4_1 = self.down_4_1(input_4th_1)\n        down_4_2 = self.down_4_2(input_4th_2)\n        down_4_3 = self.down_4_3(input_4th_3)\n\n        # Bridge\n        down_4_0m = self.pool_4_0(down_4_0)\n        down_4_1m = self.pool_4_1(down_4_1)\n        down_4_2m = self.pool_4_2(down_4_2)\n        down_4_3m = self.pool_4_3(down_4_3)\n\n        inputBridge = torch.cat((down_4_0m, down_4_1m, down_4_2m, down_4_3m), dim=1)\n        inputBridge = torch.cat((inputBridge, croppCenter(input_4th_0, inputBridge.shape)), dim=1)\n\n        bridge = self.bridge(inputBridge)\n\n        # Decoding path\n        skip_1 = (down_4_0 + down_4_1 + down_4_2 + down_4_3) / 4.0\n        skip_2 = (down_3_0 + down_3_1 + down_3_2 + down_3_3) / 4.0\n        skip_3 = (down_2_0 + down_2_1 + down_2_2 + down_2_3) / 4.0\n        skip_4 = (down_1_0 + down_1_1 + down_1_2 + down_1_3) / 4.0\n\n        x = self.upLayer1(bridge, skip_1)\n        x = self.upLayer2(x, skip_2)\n        x = self.upLayer3(x, skip_3)\n        x = self.upLayer4(x, skip_4)\n\n        return x\n\n\nclass LSTM0(nn.Module):\n    def __init__(self, in_c=5, ngf=32):\n        super(LSTM0, self).__init__()\n        # Using ConvBlock with Identity activation to emulate linear conv\n        identity = nn.Identity()\n        self.conv_gx_lstm0 = ConvBlock(in_c + ngf, ngf, identity)\n        self.conv_ix_lstm0 = ConvBlock(in_c + ngf, ngf, identity)\n        self.conv_ox_lstm0 = ConvBlock(in_c + ngf, ngf, identity)\n\n    def forward(self, xt):\n        # xt: bz * (5 + 32) * H * W\n        gx = torch.tanh(self.conv_gx_lstm0(xt))\n        ix = torch.sigmoid(self.conv_ix_lstm0(xt))\n        ox = torch.sigmoid(self.conv_ox_lstm0(xt))\n\n        cell_1 = torch.tanh(gx * ix)\n        hide_1 = ox * cell_1\n        return cell_1, hide_1\n\n\nclass LSTM(nn.Module):\n    def __init__(self, in_c=5, ngf=32):\n        super(LSTM, self).__init__()\n        identity = nn.Identity()\n        \n        # For LSTM we have pairs of conv: one on xt and one on hide_t_1.\n        # We'll create two ConvBlock layers and sum their outputs.\n        # We'll store them as (conv_ix_lstm, conv_ih_lstm, ...) using ConvBlock:\n        \n        self.conv_ix_lstm = ConvBlock(in_c + ngf, ngf, identity)\n        self.conv_ih_lstm = ConvBlock(ngf, ngf, identity)\n\n        self.conv_fx_lstm = ConvBlock(in_c + ngf, ngf, identity)\n        self.conv_fh_lstm = ConvBlock(ngf, ngf, identity)\n\n        self.conv_ox_lstm = ConvBlock(in_c + ngf, ngf, identity)\n        self.conv_oh_lstm = ConvBlock(ngf, ngf, identity)\n\n        self.conv_gx_lstm = ConvBlock(in_c + ngf, ngf, identity)\n        self.conv_gh_lstm = ConvBlock(ngf, ngf, identity)\n\n    def forward(self, xt, cell_t_1, hide_t_1):\n        gx = self.conv_gx_lstm(xt)\n        gh = self.conv_gh_lstm(hide_t_1)\n        g_sum = gx + gh\n        gt = torch.tanh(g_sum)\n\n        ox = self.conv_ox_lstm(xt)\n        oh = self.conv_oh_lstm(hide_t_1)\n        o_sum = ox + oh\n        ot = torch.sigmoid(o_sum)\n\n        ix = self.conv_ix_lstm(xt)\n        ih = self.conv_ih_lstm(hide_t_1)\n        i_sum = ix + ih\n        it = torch.sigmoid(i_sum)\n\n        fx = self.conv_fx_lstm(xt)\n        fh = self.conv_fh_lstm(hide_t_1)\n        f_sum = fx + fh\n        ft = torch.sigmoid(f_sum)\n\n        cell_t = ft * cell_t_1 + it * gt\n        hide_t = ot * torch.tanh(cell_t)\n\n        return cell_t, hide_t\n\n\nclass LSTM_MMUnet(nn.Module):\n    def __init__(self, input_nc=1, output_nc=5, ngf=32, temporal=3):\n        super(LSTM_MMUnet, self).__init__()\n        self.temporal = temporal\n        self.mmunet = MMUnet(input_nc, output_nc, ngf)\n        self.lstm0 = LSTM0(in_c=output_nc, ngf=ngf)\n        self.lstm = LSTM(in_c=output_nc, ngf=ngf)\n\n        # Using ConvBlock to replace direct Conv2d layers for output\n        # For the output convolution, we can still use ConvBlock with Identity and then a final activation outside if needed.\n        identity = nn.Identity()\n\n        # mmout and out: originally were Conv2d. Now replaced with ConvBlock followed by extracting just conv from it.\n        # However, we must produce final output without BN/activation. Let's just use ConvBlock and note that it introduces BN.\n        # If strictly needed linear: we can add Identity after block and rely on user's acceptance of BN here.\n        self.mmout_block = ConvBlock(ngf, output_nc, identity)\n        self.out_block = ConvBlock(ngf, output_nc, identity)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: bz * temporal * 4 * H * W\n        \"\"\"\n        output = []\n        mm_output = []\n        cell = None\n        hide = None\n        for t in range(self.temporal):\n            im_t = x[:, t, :, :, :]\n            mm_last = self.mmunet(im_t)        # bz * ngf(32) * H * W\n            out_t_mm = self.mmout_block(mm_last)  # bz * output_nc(5) * H * W\n            mm_output.append(out_t_mm)\n\n            lstm_in = torch.cat((out_t_mm, mm_last), dim=1) # bz * (5+32) * H * W\n\n            if t == 0:\n                cell, hide = self.lstm0(lstm_in)\n            else:\n                cell, hide = self.lstm(lstm_in, cell, hide)\n\n            out_t = self.out_block(hide)\n            output.append(out_t)\n\n        return torch.stack(mm_output, dim=1), torch.stack(output, dim=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T14:00:11.837467Z","iopub.execute_input":"2024-12-17T14:00:11.837715Z","iopub.status.idle":"2024-12-17T14:00:11.870321Z","shell.execute_reply.started":"2024-12-17T14:00:11.837691Z","shell.execute_reply":"2024-12-17T14:00:11.869515Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Hyperparameters\nnum_epochs = 5\nlearning_rate = 1e-4\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Instantiate model, loss, and optimizer\nmodel = LSTM_MMUnet(input_nc=1, output_nc=5, ngf=32, temporal=4).to(device)\n\nweight = torch.from_numpy(train_dataset.weight).float()    # weight for all classes \nweight = weight.to(device)\ncriterion = nn.CrossEntropyLoss(weight = weight)  \noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# The model was largely trained in parts. We save the models state after each epoch and use it at the start of the new cycle\n# This was done as we had computational limits and each epoch takes approximately 2.5 hours\ncheckpoint_path = '/kaggle/input/lstm_epoch0/pytorch/default/1/checkpoint_epoch_0.pth' \n\nstart_epoch = 0\nif os.path.exists(checkpoint_path):\n    # Load the checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1  # resume from next epoch\n    print(f\"Resuming training from epoch {start_epoch}\")\n\nfor epoch in range(start_epoch, num_epochs):\n    model.train()\n    train_loss = 0.0\n    for batch_idx, (images, labels, names) in enumerate(train_loader):\n        if(batch_idx%100 == 0):\n            print(f\"\\tProcessing batch {batch_idx + 1}/{len(train_loader)}\")\n        # images: (B, temporal, 4, H, W)\n        # labels: (B, temporal, H, W)\n        images = images.to(device)  # (B, T, 4, H, W)\n        labels = labels.to(device)  # (B, T, H, W)\n\n        optimizer.zero_grad()\n\n        mm_output, output = model(images)  \n        # output: (B, T, 5, H, W), labels: (B, T, H, W)\n\n        # Rearrange or pick the appropriate dimension for loss\n        # CrossEntropyLoss expects (N, C, ...) and label as (N, ...)\n        # Here, N can be B*T because we have temporal dimension as well.\n        B, T, C, H, W = output.shape\n        output_flat = output.view(B*T, C, H, W)   # (B*T, 5, H, W)\n        labels_flat = labels.view(B*T, H, W)      # (B*T, H, W)\n\n        loss = criterion(output_flat, labels_flat)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    train_loss /= len(train_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n    \n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': train_loss\n    }, f'checkpoint_epoch_{epoch}.pth')\n    \n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    temporal = 4\n    with torch.no_grad():\n        for batch_idx, (images, labels, names) in enumerate(val_loader):\n            for t in range(0, 155 - temporal, temporal):\n                image = images[:, t:t+temporal, ...]  # 5D  bs * temp * 4 * 240 * 240\n                label = labels[:, t:t+temporal, ...]       # 4D tensor   bs * temporal * 240 * 240\n                image = image.to(device)\n                label = label.to(device)\n    \n                mm_output, output = model(image)\n                B, T, C, H, W = output.shape\n                output_flat = output.view(B*T, C, H, W)\n                labels_flat = label.view(B*T, H, W)\n    \n                loss = criterion(output_flat, labels_flat)\n                val_loss += loss.item()\n                \n        val_loss /= len(val_loader)\n        \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T14:00:11.871704Z","iopub.execute_input":"2024-12-17T14:00:11.872029Z"}},"outputs":[{"name":"stdout","text":"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n ----- Creating MULTI_UNET  ...\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/3562948720.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Resuming training from epoch 1\n\tProcessing batch 1/8313\n\tProcessing batch 101/8313\n\tProcessing batch 201/8313\n\tProcessing batch 301/8313\n\tProcessing batch 401/8313\n\tProcessing batch 501/8313\n\tProcessing batch 601/8313\n\tProcessing batch 701/8313\n\tProcessing batch 801/8313\n\tProcessing batch 901/8313\n\tProcessing batch 1001/8313\n\tProcessing batch 1101/8313\n\tProcessing batch 1201/8313\n\tProcessing batch 1301/8313\n\tProcessing batch 1401/8313\n\tProcessing batch 1501/8313\n\tProcessing batch 1601/8313\n\tProcessing batch 1701/8313\n\tProcessing batch 1801/8313\n\tProcessing batch 1901/8313\n\tProcessing batch 2001/8313\n\tProcessing batch 2101/8313\n\tProcessing batch 2201/8313\n\tProcessing batch 2301/8313\n\tProcessing batch 2401/8313\n\tProcessing batch 2501/8313\n\tProcessing batch 2601/8313\n\tProcessing batch 2701/8313\n\tProcessing batch 2801/8313\n\tProcessing batch 2901/8313\n\tProcessing batch 3001/8313\n\tProcessing batch 3101/8313\n\tProcessing batch 3201/8313\n\tProcessing batch 3301/8313\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from torchinfo import summary\nsummary(model, (4,4,4, 240, 240))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(preds, labels, num_classes):\n    \"\"\"\n    Compute accuracy, dice, and F-score for multi-class segmentation.\n    preds: (N, H, W) or (B,T,H,W) predicted labels\n    labels: same shape as preds with ground truth labels\n    \"\"\"\n\n    # Ensure preds and labels are CPU numpy arrays for easy manipulation\n    preds = preds.cpu().numpy()\n    labels = labels.cpu().numpy()\n\n    # shape could be (B,T,H,W). Let's combine B and T as one dimension:\n    BTHW = preds.shape[0]*preds.shape[1] if preds.ndim == 4 else preds.shape[0]\n    if preds.ndim == 4:\n        # (B,T,H,W) => (B*T,H,W)\n        preds = preds.reshape(BTHW, preds.shape[2], preds.shape[3])\n        labels = labels.reshape(BTHW, labels.shape[2], labels.shape[3])\n    # Now preds and labels are (N,H,W)\n\n    total_voxels = np.prod(preds.shape)\n    accuracy = (preds == labels).sum() / total_voxels\n\n    # Compute dice per class\n    # Dice for class c:\n    # TP = number of voxels where pred = c and label = c\n    # FP = number of voxels where pred = c but label != c\n    # FN = number of voxels where label = c but pred != c\n    dice_scores = []\n    f_scores = []\n    dice_per_class = {}\n    f1_per_class = {}\n\n    for c in range(num_classes):\n        pred_c = (preds == c)\n        label_c = (labels == c)\n\n        # intersection = TP\n        intersection = np.logical_and(pred_c, label_c).sum()\n        pred_c_sum = pred_c.sum()\n        label_c_sum = label_c.sum()\n\n        # Dice = (2 * intersection) / (pred_c_sum + label_c_sum) if both sums not zero\n        if pred_c_sum + label_c_sum > 0:\n            dice_c = (2.0 * intersection) / (pred_c_sum + label_c_sum)\n        else:\n            dice_c = 1.0 if pred_c_sum == 0 and label_c_sum == 0 else 0.0\n\n        # F1 = 2TP/(2TP+FP+FN). Here FP = pred_c_sum - intersection, FN = label_c_sum - intersection\n        fp = pred_c_sum - intersection\n        fn = label_c_sum - intersection\n        denominator = (2*intersection + fp + fn)\n        if denominator > 0:\n            f1_c = 2*intersection / denominator\n        else:\n            f1_c = 1.0 if intersection == 0 and pred_c_sum == 0 and label_c_sum == 0 else 0.0\n\n        dice_scores.append(dice_c)\n        f_scores.append(f1_c)\n        dice_per_class[c] = dice_c\n        f1_per_class[c] = f1_c\n\n    dice_avg = np.mean(dice_scores)\n    f1_avg = np.mean(f_scores)\n\n    return accuracy, dice_avg, f1_avg, dice_per_class, f1_per_class\n\n# Test Loop\nmodel.eval()\ntest_loss = 0.0\ntest_acc = 0.0\ntest_dice = 0.0\ntest_f1 = 0.0\ncount = 0\n\n# To accumulate per-class metrics\nnum_classes = 4\ndice_per_class_sum = np.zeros(num_classes)\nf1_per_class_sum = np.zeros(num_classes)\ntemporal = 4\n\nwith torch.no_grad():\n    for batch_idx, (images, labels, names) in enumerate(test_loader):\n        \n        for t in range(0, 155 - temporal, temporal):\n            image = images[:, t:t+temporal, ...]  # 5D  bs * temp * 4 * 240 * 240\n            label = labels[:, t:t+temporal, ...]       # 4D tensor   bs * temporal * 240 * 240\n            image = image.to(device)\n            label = label.to(device)\n\n            mm_output, output = model(image)\n            B, T, C, H, W = output.shape\n            output_flat = output.view(B*T, C, H, W)\n            labels_flat = label.view(B*T, H, W)\n            \n            loss = criterion(output_flat, labels_flat)\n            test_loss += loss.item()\n\n            predictions = torch.argmax(output_flat, dim=1)  # (B*T,H,W)\n            predictions = predictions.view(B, T, H, W)\n            labs = labels_flat.view(B, T, H, W)\n\n            acc, dice_avg, f1_avg, dice_per_class, f1_per_class = compute_metrics(predictions, labs, num_classes=C)\n            test_acc += acc\n            test_dice += dice_avg\n            test_f1 += f1_avg\n\n            # Accumulate per-class metrics\n            for c in range(num_classes):\n                dice_per_class_sum[c] += dice_per_class[c]\n                f1_per_class_sum[c] += f1_per_class[c]\n\n            count += 1\n\n# Compute averages\ntest_loss /= len(test_loader)\ntest_acc /= count\ntest_dice /= count\ntest_f1 /= count\ndice_per_class_avg = dice_per_class_sum / count\nf1_per_class_avg = f1_per_class_sum / count\n\n# Print overall metrics\nprint(f\"Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, Dice: {test_dice:.4f}, F1: {test_f1:.4f}\")\n\n# Print per-class metrics\nprint(\"Per-class metrics:\")\nfor c in range(num_classes):\n    print(f\"Class {c}: Dice: {dice_per_class_avg[c]:.4f}, F1: {f1_per_class_avg[c]:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    for batch_idx, (images, labels, names) in enumerate(test_loader):\n        if batch_idx == 1:\n            image = images[:, 70:70+temporal, ...]  \n            label = labels[:, 70:70+temporal, ...]  \n            image = image.to(device)\n            label = label.to(device)\n\n            _, output = model(image) \n            preds = torch.argmax(output, dim=2)  # Predicted mask\n            break  # Visualize just one batch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_np = image.cpu().numpy()  # Input image\nlabel_np = label.cpu().numpy()  # Ground truth\npreds_np = preds.cpu().numpy()  # Predicted mask","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_idx = 1\ntemporal_idx = 0  \nmodal_idx = 0 \nspecific_image = image_np[batch_idx, temporal_idx, modal_idx]\n\n# Visualization\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Specific Temporal Slice and Channel\naxes[0].imshow(specific_image, cmap='gray')\naxes[0].set_title(f'Input image (T={temporal_idx}, M={modal_idx})')\n\n# Ground Truth and Prediction \naxes[1].imshow(label_np[batch_idx, temporal_idx], cmap='viridis')\naxes[1].set_title('Ground Truth')\n\n# Visualize Predictions \naxes[2].imshow(preds_np[batch_idx, 0], cmap='viridis')  # First temporal slice of predictions\naxes[2].set_title(f'Prediction (Batch {batch_idx})')\n\nfor ax in axes:\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select a batch\nbatch_idx = 3\ntemporal_idx = 0  # Choose a specific temporal slice\nmodal_idx = 0  # Choose a specific modality\nspecific_image = image_np[batch_idx, temporal_idx, modal_idx]\n\n# Visualization\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Specific Temporal Slice and Channel\naxes[0].imshow(specific_image, cmap='gray')\naxes[0].set_title(f'Input image (T={temporal_idx}, M={modal_idx})')\n\n# Ground Truth and Prediction \naxes[1].imshow(label_np[batch_idx, temporal_idx], cmap='viridis')\naxes[1].set_title('Ground Truth')\n\n# Visualize Predictions \naxes[2].imshow(preds_np[batch_idx, 0], cmap='viridis')  # First temporal slice of predictions\naxes[2].set_title(f'Prediction (Batch {batch_idx})')\n\nfor ax in axes:\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}