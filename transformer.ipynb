{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2542390,"sourceType":"datasetVersion","datasetId":1541666}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tarfile\nimport os\n\n# Path to the TAR file\ntar_path = '/kaggle/input/brats-2021-task1/BraTS2021_Training_Data.tar'\nextract_path = '/kaggle/working/BraTS2021_Training_Data'\n\n# Extract the TAR file\nwith tarfile.open(tar_path, 'r') as tar:\n    tar.extractall(path=extract_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T09:11:12.951619Z","iopub.execute_input":"2024-12-13T09:11:12.952000Z","iopub.status.idle":"2024-12-13T09:12:44.931572Z","shell.execute_reply.started":"2024-12-13T09:11:12.951952Z","shell.execute_reply":"2024-12-13T09:12:44.930456Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import nibabel as nib\nimport numpy as np\nimport os\n\n\n# Path to the TAR file\ntar_path = '/kaggle/input/brats-2021-task1/BraTS2021_Training_Data.tar'\nextract_path = '/kaggle/working/BraTS2021_Training_Data'\n\n\n\ndef load_nifti(file_path):\n    \"\"\"\n    Load a NIfTI file and convert it to a NumPy array.\n    \"\"\"\n    img = nib.load(file_path)\n    return img.get_fdata()\n\ndef preprocess_modality(modality_path):\n    \"\"\"\n    Normalize the modality data.\n    \"\"\"\n    data = load_nifti(modality_path)\n    # Normalize (z-score)\n    mean, std = data.mean(), data.std()\n    return (data - mean) / std\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-13T09:12:44.933345Z","iopub.execute_input":"2024-12-13T09:12:44.934019Z","iopub.status.idle":"2024-12-13T09:12:45.425841Z","shell.execute_reply.started":"2024-12-13T09:12:44.933977Z","shell.execute_reply":"2024-12-13T09:12:45.425141Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nimport os\n\n# Dataset Class\nclass BraTSDataset(Dataset):\n    def __init__(self, root_dir, num_classes, transform=None):\n        self.root_dir = root_dir\n        self.num_classes = num_classes\n        self.transform = transform\n        self.patients = [\n            folder for folder in sorted(os.listdir(root_dir))\n            if not folder.startswith('.')\n        ]\n\n    def __len__(self):\n        return len(self.patients)\n\n    def __getitem__(self, idx):\n        patient_dir = os.path.join(self.root_dir, self.patients[idx])\n    \n        # Load modalities\n        t1 = preprocess_modality(os.path.join(patient_dir, f\"{self.patients[idx]}_t1.nii.gz\"))\n        t1ce = preprocess_modality(os.path.join(patient_dir, f\"{self.patients[idx]}_t1ce.nii.gz\"))\n        t2 = preprocess_modality(os.path.join(patient_dir, f\"{self.patients[idx]}_t2.nii.gz\"))\n        flair = preprocess_modality(os.path.join(patient_dir, f\"{self.patients[idx]}_flair.nii.gz\"))\n    \n        # Load segmentation and ensure valid range\n        seg = load_nifti(os.path.join(patient_dir, f\"{self.patients[idx]}_seg.nii.gz\"))\n        seg = torch.tensor(seg, dtype=torch.long)\n        seg = torch.clamp(seg, min=0, max=self.num_classes - 1)  # Ensure valid range\n    \n        # One-hot encode\n        seg_one_hot = F.one_hot(seg, num_classes=self.num_classes).permute(3, 0, 1, 2).float()\n    \n        # Combine modalities into a tensor\n        image = np.stack([t1, t1ce, t2, flair], axis=0)\n        image = torch.tensor(image, dtype=torch.float32)\n    \n        return image, seg_one_hot\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-13T09:12:45.426895Z","iopub.execute_input":"2024-12-13T09:12:45.427243Z","iopub.status.idle":"2024-12-13T09:12:49.820936Z","shell.execute_reply.started":"2024-12-13T09:12:45.427216Z","shell.execute_reply":"2024-12-13T09:12:49.820027Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, in_channels, patch_size, embed_dim):\n        super().__init__()\n        self.patch_size = patch_size\n        self.projection = nn.Conv3d(\n            in_channels, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        x = self.projection(x)  # (B, embed_dim, D, H, W)\n        self.d, self.h, self.w = x.shape[2:]  # Store dimensions for later use\n        B, C, D, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, feedforward_dim):\n        super(TransformerBlock, self).__init__()\n        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n        self.feedforward = nn.Sequential(\n            nn.Linear(embed_dim, feedforward_dim),\n            nn.ReLU(),\n            nn.Linear(feedforward_dim, embed_dim)\n        )\n        self.layernorm1 = nn.LayerNorm(embed_dim)\n        self.layernorm2 = nn.LayerNorm(embed_dim)\n    \n    def forward(self, x):\n        # Self-attention\n        attn_output, _ = self.attention(x, x, x)\n        x = self.layernorm1(x + attn_output)\n        \n        # Feedforward\n        ff_output = self.feedforward(x)\n        x = self.layernorm2(x + ff_output)\n        return x\n\nclass TransformerSegmentation(nn.Module):\n    def __init__(\n        self,\n        input_channels,\n        patch_size,\n        embed_dim,\n        depth,\n        heads,\n        mlp_dim,\n        dropout_rate=0.1,\n        attn_dropout_rate=0.1,\n        num_classes=4,\n    ):\n        super().__init__()\n        self.embedding = PatchEmbedding(\n            in_channels=input_channels, patch_size=patch_size, embed_dim=embed_dim\n        )\n        self.transformer = nn.Sequential(\n            *[\n                TransformerBlock(embed_dim, heads, mlp_dim)\n                for _ in range(depth)\n            ]\n        )\n        self.output_projection = nn.Conv3d(embed_dim, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        # Embed patches\n        x = self.embedding(x)\n\n        # Process with transformer\n        x = self.transformer(x)\n\n        # Reshape back to spatial dimensions\n        B, num_patches, embed_dim = x.shape\n        x = x.transpose(1, 2).reshape(B, embed_dim, self.embedding.d, self.embedding.h, self.embedding.w)\n        x = self.output_projection(x)  # Final segmentation output\n        return x\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-13T09:12:49.822880Z","iopub.execute_input":"2024-12-13T09:12:49.823302Z","iopub.status.idle":"2024-12-13T09:12:49.834151Z","shell.execute_reply.started":"2024-12-13T09:12:49.823274Z","shell.execute_reply":"2024-12-13T09:12:49.833439Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torch.optim import Adam\nimport torch.nn.functional as F\n\n\nclass DiceLoss(nn.Module):\n    def __init__(self):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, logits, targets):\n        smooth = 1.0\n\n        # Apply softmax to logits to get probabilities\n        probs = torch.softmax(logits, dim=1)\n\n        # Compute Dice loss\n        intersection = (probs * targets).sum(dim=(2, 3, 4))\n        union = probs.sum(dim=(2, 3, 4)) + targets.sum(dim=(2, 3, 4))\n        dice = (2.0 * intersection + smooth) / (union + smooth)\n\n        return 1 - dice.mean()\n\n\ndef train_transbts(model, train_loader, val_loader, epochs=20, lr=1e-4, device='cuda'):\n    model.to(device)\n    optimizer = Adam(model.parameters(), lr=lr)\n    criterion = CombinedLoss()\n    \n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for images, masks in train_loader:\n            images, masks = images.to(device), masks.to(device)\n            \n            outputs = model(images)\n            outputs = F.interpolate(outputs, size=masks.shape[1:], mode=\"trilinear\", align_corners=False)\n            loss = criterion(outputs, masks)\n            train_loss += loss.item()\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-13T09:12:49.835178Z","iopub.execute_input":"2024-12-13T09:12:49.835520Z","iopub.status.idle":"2024-12-13T09:12:49.851769Z","shell.execute_reply.started":"2024-12-13T09:12:49.835489Z","shell.execute_reply":"2024-12-13T09:12:49.850958Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n\n       \n\n# Training Function\ndef train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    epoch_loss = 0\n\n    for images, masks in loader:\n        images, masks = images.to(device), masks.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        outputs = F.interpolate(outputs, size=masks.shape[2:], mode=\"trilinear\", align_corners=False)\n        \n        # Compute loss\n        loss = criterion(outputs, masks)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    return epoch_loss / len(loader)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T09:12:49.853248Z","iopub.execute_input":"2024-12-13T09:12:49.853589Z","iopub.status.idle":"2024-12-13T09:12:49.865229Z","shell.execute_reply.started":"2024-12-13T09:12:49.853551Z","shell.execute_reply":"2024-12-13T09:12:49.864314Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    val_loss = 0\n    dice_coeff = 0\n\n    with torch.no_grad():\n        for images, masks in loader:\n            images, masks = images.to(device), masks.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            outputs = F.interpolate(outputs, size=masks.shape[2:], mode=\"trilinear\", align_corners=False)\n\n            # Compute validation loss\n            val_loss += criterion(outputs, masks).item()\n\n            # Dice coefficient\n            probs = torch.softmax(outputs, dim=1)\n            preds = (probs > 0.5).float()\n            intersection = (preds * masks).sum()\n            dice_coeff += (2.0 * intersection) / (preds.sum() + masks.sum() + 1e-5)\n\n    return val_loss / len(loader), dice_coeff / len(loader)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-13T09:12:49.866198Z","iopub.execute_input":"2024-12-13T09:12:49.866479Z","iopub.status.idle":"2024-12-13T09:12:49.881210Z","shell.execute_reply.started":"2024-12-13T09:12:49.866454Z","shell.execute_reply":"2024-12-13T09:12:49.880541Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\n\nfrom torch.utils.data import Subset\nimport random\n\n\n# Split the dataset into training and validation sets\n# Define the number of classes in your segmentation task\nnum_classes = 4  # Replace 4 with the actual number of classes in your dataset\n\n# Initialize the dataset\ndataset = BraTSDataset(root_dir=extract_path, num_classes=num_classes)\ntrain_idx, val_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n\n# Select a subset of the dataset\n#subset_size = 10  # Number of samples to use for debugging\n#indices = random.sample(range(len(dataset)), subset_size)  # Randomly sample indices\n#debug_dataset = Subset(dataset, indices)\n\ntrain_dataset = torch.utils.data.Subset(dataset, train_idx)\nval_dataset = torch.utils.data.Subset(dataset, val_idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)\n\n# Create DataLoaders\n#train_loader = DataLoader(debug_dataset, batch_size=1, shuffle=True, num_workers=4)\n#val_loader = DataLoader(debug_dataset, batch_size=1, shuffle=False, num_workers=4)\n\n\n# Debug the DataLoader\nfor images, masks in train_loader:\n    print(\"Image batch shape:\", images.shape)\n    print(\"Mask batch shape:\", masks.shape)\n    print(\"Mask unique values:\", torch.unique(masks))\n    break  # Debug only one batch\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T09:12:49.882224Z","iopub.execute_input":"2024-12-13T09:12:49.882501Z","iopub.status.idle":"2024-12-13T09:13:02.693601Z","shell.execute_reply.started":"2024-12-13T09:12:49.882474Z","shell.execute_reply":"2024-12-13T09:13:02.692438Z"}},"outputs":[{"name":"stdout","text":"Image batch shape: torch.Size([1, 4, 240, 240, 155])\nMask batch shape: torch.Size([1, 4, 240, 240, 155])\nMask unique values: tensor([0., 1.])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n\n\nfrom torch.optim import Adam\n\n# Training setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TransformerSegmentation(\n    input_channels=4, patch_size=4, embed_dim=128, depth=6, heads=8, mlp_dim=512, num_classes=4\n).to(device)\n\n# Free GPU cache\ntorch.cuda.empty_cache()\n\n# Train with reduced batch size\nbatch_size = 1\n\n\n# Initialize optimizer and loss function\noptimizer = Adam(model.parameters(), lr=1e-4)\ncriterion = DiceLoss()\n\n# Training loop\nnum_epochs = 13\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    val_loss, dice_score = evaluate(model, val_loader, criterion, device)\n    \n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    print(f\"Train Loss: {train_loss:.4f}\")\n    print(f\"Val Loss: {val_loss:.4f}, Dice Score: {dice_score:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-12-13T09:13:02.695137Z","iopub.execute_input":"2024-12-13T09:13:02.695805Z","iopub.status.idle":"2024-12-13T14:51:34.082134Z","shell.execute_reply.started":"2024-12-13T09:13:02.695773Z","shell.execute_reply":"2024-12-13T14:51:34.080909Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/13\nTrain Loss: 0.4740\nVal Loss: 0.4189, Dice Score: 0.9930\nEpoch 2/13\nTrain Loss: 0.4134\nVal Loss: 0.4052, Dice Score: 0.9913\nEpoch 3/13\nTrain Loss: 0.4024\nVal Loss: 0.3720, Dice Score: 0.9934\nEpoch 4/13\nTrain Loss: 0.3820\nVal Loss: 0.3590, Dice Score: 0.9935\nEpoch 5/13\nTrain Loss: 0.3653\nVal Loss: 0.3563, Dice Score: 0.9938\nEpoch 6/13\nTrain Loss: 0.3587\nVal Loss: 0.3473, Dice Score: 0.9950\nEpoch 7/13\nTrain Loss: 0.3462\nVal Loss: 0.3600, Dice Score: 0.9913\nEpoch 8/13\nTrain Loss: 0.3474\nVal Loss: 0.3434, Dice Score: 0.9949\nEpoch 9/13\nTrain Loss: 0.3423\nVal Loss: 0.3778, Dice Score: 0.9922\nEpoch 10/13\nTrain Loss: 0.3361\nVal Loss: 0.3187, Dice Score: 0.9949\nEpoch 11/13\nTrain Loss: 0.3308\nVal Loss: 0.3258, Dice Score: 0.9947\nEpoch 12/13\nTrain Loss: 0.3384\nVal Loss: 0.3134, Dice Score: 0.9953\nEpoch 13/13\nTrain Loss: 0.3200\nVal Loss: 0.3405, Dice Score: 0.9942\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/transbts_model.pth')\nprint(\"Model saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:51:34.086340Z","iopub.execute_input":"2024-12-13T14:51:34.087092Z","iopub.status.idle":"2024-12-13T14:51:34.112219Z","shell.execute_reply.started":"2024-12-13T14:51:34.087058Z","shell.execute_reply":"2024-12-13T14:51:34.111430Z"}},"outputs":[{"name":"stdout","text":"Model saved!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Load the model\nmodel.load_state_dict(torch.load('/kaggle/working/transbts_model.pth'))\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:51:34.113169Z","iopub.execute_input":"2024-12-13T14:51:34.113466Z","iopub.status.idle":"2024-12-13T14:51:34.142493Z","shell.execute_reply.started":"2024-12-13T14:51:34.113433Z","shell.execute_reply":"2024-12-13T14:51:34.141657Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_73/2139254807.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/transbts_model.pth'))\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TransformerSegmentation(\n  (embedding): PatchEmbedding(\n    (projection): Conv3d(4, 128, kernel_size=(4, 4, 4), stride=(4, 4, 4))\n  )\n  (transformer): Sequential(\n    (0): TransformerBlock(\n      (attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n      )\n      (feedforward): Sequential(\n        (0): Linear(in_features=128, out_features=512, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=512, out_features=128, bias=True)\n      )\n      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): TransformerBlock(\n      (attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n      )\n      (feedforward): Sequential(\n        (0): Linear(in_features=128, out_features=512, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=512, out_features=128, bias=True)\n      )\n      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): TransformerBlock(\n      (attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n      )\n      (feedforward): Sequential(\n        (0): Linear(in_features=128, out_features=512, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=512, out_features=128, bias=True)\n      )\n      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): TransformerBlock(\n      (attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n      )\n      (feedforward): Sequential(\n        (0): Linear(in_features=128, out_features=512, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=512, out_features=128, bias=True)\n      )\n      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    )\n    (4): TransformerBlock(\n      (attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n      )\n      (feedforward): Sequential(\n        (0): Linear(in_features=128, out_features=512, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=512, out_features=128, bias=True)\n      )\n      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    )\n    (5): TransformerBlock(\n      (attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n      )\n      (feedforward): Sequential(\n        (0): Linear(in_features=128, out_features=512, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=512, out_features=128, bias=True)\n      )\n      (layernorm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (layernorm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (output_projection): Conv3d(128, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}